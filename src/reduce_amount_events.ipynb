{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.patches as mpatches\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import numpy as np\n",
    "from utils import utils\n",
    "\n",
    "\n",
    "url_data = \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "2\n",
      "8\n",
      "6\n",
      "15\n",
      "2\n",
      "8\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "speed_high = 15\n",
    "speed_low = 2\n",
    "speed_moderate_down = 6\n",
    "speed_moderate_up = 8\n",
    "utils.set_speed(speed_high,speed_low,speed_moderate_down,speed_moderate_up)\n",
    "\n",
    "print(utils.speed_high)\n",
    "print(utils.speed_low)\n",
    "print(utils.speed_moderate_up)\n",
    "print(utils.speed_moderate_down)\n",
    "\n",
    "utils.get_speeds()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count number of events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 2280\n",
      "2001 6420\n",
      "2002 2930\n",
      "2009 4560\n",
      "2010 7679\n",
      "2013 6529\n",
      "2014 9046\n",
      "2017 5008\n",
      "2019 5775\n",
      "2020 2052\n"
     ]
    }
   ],
   "source": [
    "range_cut = 96\n",
    "path_save = os.path.join(url_data,\"dataset_split_events_rare\")\n",
    "\n",
    "years = [\"2000\",\"2001\",\"2002\",\"2003\",\"2004\",\"2005\",\"2006\",\"2007\",\"2008\",\"2009\",\\\n",
    "         \"2010\",\"2011\",\"2012\",\"2013\",\"2014\",\"2015\",\"2016\",\"2017\",\"2018\",\"2019\",\"2020\"]\n",
    "\n",
    "for year in years:\n",
    "    try:\n",
    "        print(year,utils.count_event_per_file(os.path.join(url_data,\"dataset_summer_split_13\",f\"{year}_{range_cut}_13_high_summer.csv\"))['events'].sum())\n",
    "    except:\n",
    "        pass\n",
    "    # try:\n",
    "    #     print(utils.count_event_per_file(os.path.join(url_data,\"dataset_split_events\",f\"{year}_{range_cut}_moderate.csv\"))['events'].sum())\n",
    "    # except:\n",
    "    #     print(\"empty csv\")\n",
    "    # try:\n",
    "    #     print(utils.count_event_per_file(os.path.join(url_data,\"dataset_split_events\",f\"{year}_{range_cut}_low.csv\"))['events'].sum())\n",
    "    # except:\n",
    "    #     print(\"empty csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split summer and winter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "ini_winder = datetime.strptime(\"2000-01-01 00:00:00\",'%Y-%m-%d %H:%M:%S')\n",
    "end_winter = datetime.strptime(\"2000-06-30 23:59:59\",'%Y-%m-%d %H:%M:%S')\n",
    "ini_summer = datetime.strptime(\"2000-07-01 00:00:00\",'%Y-%m-%d %H:%M:%S')\n",
    "end_summer = datetime.strptime(\"2000-12-31 23:59:59\",'%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "md_ini_winter = (ini_winder.month, ini_winder.day)\n",
    "md_end_winter = (end_winter.month, end_winter.day)\n",
    "md_ini_summer = (ini_summer.month, ini_summer.day)\n",
    "md_end_summer = (end_summer.month, end_summer.day)\n",
    "\n",
    "for file in os.listdir(\"../data/dataset_split_events_15/\"):\n",
    "    df = pl.read_csv(\"../data/dataset_split_events_15/\" + file)\n",
    "\n",
    "    cut_size = 96\n",
    "\n",
    "    middle = int(cut_size/2)\n",
    "\n",
    "    winter_df = pl.DataFrame()\n",
    "    summer_df = pl.DataFrame()\n",
    "\n",
    "    for windfarm in range(0,38):\n",
    "        # print(\"windfarm nÂº\",windfarm)\n",
    "        df_new = df.filter(pl.col(\"index\") == windfarm)\n",
    "        n_events = df_new[\"n_event\"].unique().shape[0]\n",
    "        # Compute number of rows in summer and in winter for the events in the middle\n",
    "\n",
    "        count_winter = 0\n",
    "        count_summer = 0\n",
    "\n",
    "        for event in range(0,n_events):\n",
    "            df_new_event = df_new.filter(pl.col(\"n_event\") == event)\n",
    "            time_split = datetime.strptime(df_new_event[middle,0],'%Y-%m-%d %H:%M:%S')\n",
    "            date_md = (time_split.month,time_split.day)\n",
    "\n",
    "            if md_ini_winter <= date_md <= md_end_winter:\n",
    "                df_new_event = df_new_event.with_column(pl.lit(count_winter).alias(\"n_event\"))\n",
    "                winter_df = pl.concat([winter_df,df_new_event], rechunk=True)\n",
    "                count_winter += 1 \n",
    "            else:\n",
    "                df_new_event = df_new_event.with_column(pl.lit(count_summer).alias(\"n_event\"))\n",
    "                summer_df = pl.concat([summer_df,df_new_event], rechunk=True)\n",
    "                count_summer += 1 \n",
    "\n",
    "    summer_df.write_csv(os.path.join(f\"../data/dataset_summer_split_15/\",f\"{file[:-4]}summer.csv\"))\n",
    "    winter_df.write_csv(os.path.join(f\"../data/dataset_winter_split_15/\",f\"{file[:-4]}winter.csv\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create list of files by "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "['../../../data/file_per_event/current_experiment/0_0_2000', '../../../data/file_per_event/current_experiment/0_102_2000', '../../../data/file_per_event/current_experiment/0_103_2000', '../../../data/file_per_event/current_experiment/0_10_2000', '../../../data/file_per_event/current_experiment/0_113_2000', '../../../data/file_per_event/current_experiment/0_11_2000', '../../../data/file_per_event/current_experiment/0_12_2000', '../../../data/file_per_event/current_experiment/0_13_2000', '../../../data/file_per_event/current_experiment/0_14_2000', '../../../data/file_per_event/current_experiment/0_15_2000', '../../../data/file_per_event/current_experiment/0_17_2000', '../../../data/file_per_event/current_experiment/0_19_2000', '../../../data/file_per_event/current_experiment/0_16_2000', '../../../data/file_per_event/current_experiment/0_18_2000']\n",
      "\n",
      "['0.0 1.002 0.0 0.0 -0.0 0.001 0.0 0.015 0.0 -0.016 0.0 0.0 -0.0', '0.0 1.0 0.0 0.0 0.0 -0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0', '0.0 1.0 0.0 -0.0 0.0 0.0 0.0 0.0 0.0 -0.0 -0.0 0.0 0.0', '0.0 0.0 0.0 0.007 2.39 -0.347 0.0 0.0 0.0 1.275 0.059 -156.294 156.295', '0.0 2599.489 0.0 0.0 0.0 0.499 -2606.725 0.0 0.0 0.0 1.727 0.0 0.0', '0.0 0.0 -25.997 0.0 -0.0 -0.012 26.049 4.704 -3.748 0.0 0.001 0.0 0.004', '105.392 -110.279 0.0 -0.064 -0.002 4.247 0.0 0.0 0.0 0.0 0.089 1.678 0.0', '0.0 0.998 0.0 -0.0 0.0 0.036 0.001 0.0 -0.576 0.541 -0.0 -0.0 0.0', '0.0 0.0 -0.0 0.0 0.0 0.0 0.0 0.0 3.8 -2.8 0.0 0.0 0.0', '0.0 0.0 0.0 0.0 0.0 0.0 -0.0 0.0 0.263 0.737 0.0 0.0 0.0', '0.0 -0.0 0.0 0.0 0.0 0.0 0.0 0.0 -0.357 1.357 -0.0 0.0 0.0', '0.0 365.893 0.0 0.058 0.072 21.75 0.699 -371.243 0.0 -24.816 0.0 6.291 0.0', '0.0 -0.044 0.0 -0.0 -0.0 0.0 0.0 0.0 0.0 0.0 0.0 -0.0 1.044', '0.0 0.042 0.0 0.0 0.0 0.0 0.0 0.0 -0.0 0.0 0.0 0.0 0.958']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = \"./IKM/clustering/\"\n",
    "file_clusters = \"cluster_split.txt\"\n",
    "\n",
    "with open(os.path.join(path,file_clusters), 'r') as file:\n",
    "    # Lee el contenido del archivo\n",
    "    data = file.readlines()\n",
    "\n",
    "new_data = []\n",
    "for line in data:\n",
    "    new_data.append(line.strip().split('\\n')[0])\n",
    "\n",
    "index_1 = new_data.index('Cl#:0')\n",
    "index_2 = new_data.index('Cl#:1')\n",
    "index_3 = new_data.index('Cl#:2')\n",
    "index_4 = new_data.index('Coefficients')\n",
    "\n",
    "cluster_1 = new_data[index_1+1 : index_2]\n",
    "cluster_2 = new_data[index_2+1 : index_3]\n",
    "cluster_3 = new_data[index_3+1 : index_4]\n",
    "coeff = new_data[index_4+1 :]\n",
    "\n",
    "\n",
    "\n",
    "for index,info in enumerate(cluster_1):\n",
    "    cluster_1[index] = \"../../../data/file_per_event/current_experiment/\" + info + \".txt\"\n",
    "    # print(\"\\\"\" + cluster_1[index] + \".txt\\\",\")\n",
    "\n",
    "print()\n",
    "\n",
    "for index,info in enumerate(cluster_2):\n",
    "    cluster_2[index] = \"../../../data/file_per_event/current_experiment/\" + info + \".txt\"\n",
    "    # print(\"\\\"\" + cluster_2[index] + \".txt\\\",\")\n",
    "\n",
    "print()\n",
    "\n",
    "for index,info in enumerate(cluster_3):\n",
    "    cluster_3[index] = \"../../../data/file_per_event/current_experiment/\" + info + \".txt\"\n",
    "    # print(\"\\\"\" + cluster_3[index] + \".txt\\\",\")\n",
    "\n",
    "print(cluster_1)\n",
    "\n",
    "print()\n",
    "print(coeff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create file for event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "path = \"../data/final_files/small/\"\n",
    "path_save_file_per_event = \"../../../data/file_per_event/current_experiment\"\n",
    "\n",
    "def create_files_per_event(event,name,path):\n",
    "\n",
    "    if not os.path.exists(os.path.join(path,f'{name}.txt')):\n",
    "        event.write_csv(os.path.join(path,f'{name}.txt'))\n",
    "        \n",
    "    return None\n",
    "\n",
    "for file in os.listdir():\n",
    "\n",
    "    data = pl.read_csv(os.path.join(path, file))\n",
    "\n",
    "    categories = np.array(['_'.join(str(item) for item in sublist) for sublist in data[:,1:4].rows()])\n",
    "\n",
    "    # Get unique categories\n",
    "    unique_categories = np.unique(categories)\n",
    "    np.random.shuffle(unique_categories)\n",
    "\n",
    "    # Split the array into a list of lists based on categories\n",
    "    for index,category in enumerate(tqdm(unique_categories, desc='Split events, and create TSObject', leave=False)):\n",
    "        # Select the one event in a big dataframe\n",
    "        indices = np.where(categories == category)\n",
    "        subset = data[indices[0]]\n",
    "\n",
    "        create_files_per_event(subset,category,path_save_file_per_event)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
